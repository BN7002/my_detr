{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import sklearn\n",
    "# !pip install sklearn\n",
    "import sklearn.metrics\n",
    "import matcher\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb4f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75314c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "class CNNbackbone(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.resnet = list(torchvision.models.resnet34().children())[:-2]\n",
    "        self.cuttedResnet34 = nn.Sequential(*self.resnet)\n",
    "        self.cnn1 = nn.Conv2d(512, 2048, 3, 1, 1)\n",
    "        self.descaler = nn.Conv2d(2048, d, 1)\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # image input z,c,800,600\n",
    "        y = self.cuttedResnet34(input)\n",
    "        y = self.cnn1(y)\n",
    "        y = self.descaler(y)\n",
    "        return y\n",
    "class DETR(nn.Module):\n",
    "    def __init__(self, descaleDims: int, num_heads: int, num_classes: int, num_queries: int):\n",
    "        super().__init__()\n",
    "        self.backbone = CNNbackbone(descaleDims)\n",
    "        self.descale = descaleDims\n",
    "\n",
    "        self.object_queries = nn.Parameter(torch.rand(num_queries, self.descale))\n",
    "        self.col_embed = nn.Parameter(torch.rand(100, self.descale // 2))\n",
    "        self.row_embed = nn.Parameter(torch.rand(100, self.descale // 2))\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "\n",
    "        self.encoderL = nn.TransformerEncoderLayer(self.descale, self.num_heads)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=self.encoderL, num_layers=4)\n",
    "\n",
    "        self.decoderL = nn.TransformerDecoderLayer(self.descale, self.num_heads)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoderL, 4)\n",
    "\n",
    "        self.ffn1 = FeedForward(self.descale, 0.1)\n",
    "        self.ffn2 = FeedForward(self.descale, 0.1)\n",
    "        self.ffn3 = FeedForward(self.descale, 0.1)\n",
    "        \n",
    "        self.single_box_out = nn.Linear(self.descale, 4)# X, Y _n | W, H _n\n",
    "        self.categorical = nn.Linear(self.descale, num_classes + 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        pass\n",
    "    def forward(self, input: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        W_ORG, H_ORG = input.shape[-2:]\n",
    "\n",
    "        y = self.backbone(input).squeeze(0)\n",
    "        batch_size, d, H, W = (None, None, None, None)\n",
    "        if(len(y.size()) == 3):\n",
    "            y = y.unsqueeze(0)\n",
    "        if (len(y.size()) == 4):\n",
    "            batch_size, d, H, W = y.size()\n",
    "        # print(y.size())\n",
    "        # y = y.reshape(d, H*W)\n",
    "        # print(self.col_embed[:W])\n",
    "        emb_pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1)\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "        # add posistional embedding\n",
    "        # print(f\"{y.shape}, {emb_pos.shape}, {H}, {W}\")\n",
    "        # print(f\"{y.flatten(2).permute(2,0,1).shape}, {emb_pos.shape}\")\n",
    "        h = y.flatten(2).permute(2, 0, 1)\n",
    "        h = h + emb_pos\n",
    "        # y = y.unsqueeze(1).flatten(2).permute(2,0,1) + emb_pos\n",
    "        # print(f\"{h.shape}, {emb_pos.shape}\")\n",
    "        y = self.encoder.forward(h)\n",
    "        \n",
    "        tgt = self.object_queries.unsqueeze(1).repeat(1, batch_size, 1)\n",
    "        # print(f\"{tgt.shape} {y.shape}\")\n",
    "        y = self.decoder.forward(tgt, y)\n",
    "        # torch.Size([1024, 475]) d, H*W\n",
    "        # dla kazdego d\n",
    "        y = self.ffn3(self.ffn2(self.ffn1(y)))\n",
    "        \n",
    "        boxes = self.single_box_out(y)\n",
    "        classes = self.categorical(y)\n",
    "\n",
    "        return {\"pred_boxes\": self.prediction_normalized_xywh_to_x1y1x2y2(self.sigmoid(boxes), W_ORG, H_ORG),\n",
    "                \"pred_logits\": classes}\n",
    "    def prediction_normalized_xywh_to_x1y1x2y2(self, box: torch.Tensor, imgWidth: int, imgHeight: int):\n",
    "        x_n = box[..., 0]\n",
    "        y_n = box[..., 1]\n",
    "        w_n = box[..., 2]\n",
    "        h_n = box[..., 3]\n",
    "\n",
    "        x1 = x_n * imgWidth\n",
    "        y1 = y_n * imgHeight\n",
    "        x2 = x1 + w_n * imgWidth\n",
    "        y2 = y1 + h_n * imgHeight\n",
    "\n",
    "        return torch.stack([x1, y1, x2, y2], dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FIX_WIDTH = 800\n",
    "IMAGE_FIX_HEIGHT = 800\n",
    "# Define a simple transform (resize, to tensor, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_FIX_WIDTH, IMAGE_FIX_HEIGHT)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the training dataset\n",
    "coco_train = CocoDetection(\n",
    "    root='coco/train2017',\n",
    "    annFile='coco/annotations/instances_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "coco_valid = CocoDetection(\n",
    "    root='coco/val2017',\n",
    "    annFile='coco/annotations/instances_val2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of tuples (image, anns)\n",
    "      - image: Tensor[C,H,W]\n",
    "      - anns: list of dicts, each with keys 'bbox' and 'category_id' (and others)\n",
    "    Returns:\n",
    "      images: Tensor[B, C, H, W]\n",
    "      targets: list of B dicts, each with:\n",
    "        - 'boxes': Tensor[N_i, 4] in [x1,y1,x2,y2] format\n",
    "        - 'labels': Tensor[N_i]\n",
    "    \"\"\"\n",
    "    images, all_anns = zip(*batch)\n",
    "    # Stack images into [B, C, H, W]\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    targets = []\n",
    "    for anns in all_anns:\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'] - 1)  # zeroâ€‘base labels\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        targets.append({\n",
    "            'boxes': boxes,\n",
    "            'labels': labels\n",
    "        })\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "# Load into DataLoader\n",
    "train_loader = DataLoader(coco_train, batch_size=8, shuffle=True, num_workers=8, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(coco_valid, batch_size=8, shuffle=True, num_workers=8, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97de470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.box_ops import generalized_box_iou\n",
    "\n",
    "\n",
    "NUM_CLASSES = 90\n",
    "\n",
    "def train_fn(model: nn.Module, dataloader: torch.utils.data.DataLoader, epochs: int = 30):\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(DEVICE)\n",
    "    model.train()\n",
    "    loss_clasfier = nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    loss_box_matcher = matcher.HungarianMatcher(device=DEVICE).to(DEVICE)\n",
    "\n",
    "    # every accum_steps to optim step (weight update)\n",
    "    accumulation_steps = 16\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\", total=epochs):\n",
    "        model.train()\n",
    "        batch_loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch_idx, (images, targets) in enumerate(batch_loop):\n",
    "            images = images.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model.forward(images)\n",
    "            # [(tensor_indicies_boxes, tensor_indicies_clasf)]\n",
    "            # print(prediction['pred_boxes'].shape)\n",
    "            prediction['pred_boxes'] = prediction['pred_boxes'].permute(1, 0, 2)\n",
    "            prediction['pred_logits'] = prediction['pred_logits'].permute(1, 0, 2)\n",
    "            recon = loss_box_matcher.forward(prediction, targets)\n",
    "            \n",
    "\n",
    "            pred_boxes = [prediction['pred_boxes'][k, recon[k][0], :] for k in range(prediction['pred_boxes'].shape[0])]\n",
    "            tg_boxes = [targets[k]['boxes'].to(DEVICE) for k in range(len(targets))]\n",
    "            \n",
    "            pred_clasf = [prediction['pred_logits'][k, recon[k][1], :] for k in range(prediction['pred_logits'].shape[0])]\n",
    "            tg_clasf = [targets[k]['labels'].to(DEVICE) for k in range(len(targets))]\n",
    "            iou_loss = 1 - torch.mean(generalized_box_iou(torch.cat([pred_image_boxes for pred_image_boxes in pred_boxes]),\\\n",
    "                                                           torch.cat([true_image_boxes for true_image_boxes in tg_boxes])))\n",
    "            one_hot_target_class = torch.nn.functional.one_hot(torch.cat([tg for tg in tg_clasf]), num_classes=NUM_CLASSES+1)\n",
    "            loss_clasf = loss_clasfier.forward(torch.cat([pc for pc in pred_clasf]), one_hot_target_class.float())\n",
    "            total_loss = 0.0\n",
    "\n",
    "            total_loss += (iou_loss + loss_clasf) / accumulation_steps\n",
    "            batch_loop.set_postfix({\n",
    "                'Loss': f'{total_loss:.4f}',\n",
    "                'IoU': f'{1 - iou_loss.item():.4f}',\n",
    "                'Cls': f'{loss_clasf.item():.4f}'\n",
    "            })\n",
    "            # for i, targetValue in enumerate(targets):\n",
    "            #     prediction_boxes = prediction['pred_boxes'][recon[i][0]]\n",
    "            #     prediction_clasf = prediction['pred_logits'][recon[i][1]]\n",
    "            #     print(f\"{prediction['pred_boxes'].shape} {prediction_boxes.shape} , {targets[i]['boxes'].shape}\")\n",
    "            total_loss.backward()\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        torch.save(model, f\"chkpts/detr_{epoch+1}\")\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7869b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box1 = torch.tensor([[10,10,20,20], [50, 100, 200, 300]])\n",
    "# box2 = torch.tensor([[10,10,15,15], [80, 120, 220, 310]])\n",
    "# print(f\"iou: {calculate_iou(box1, box2)}\")\n",
    "\n",
    "# print(box1.shape, box2.shape)\n",
    "# box1[:, :2], box2[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detr = None\n",
    "if detr:\n",
    "    del detr\n",
    "detr = DETR(1024, 8, NUM_CLASSES, 128).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036e393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed18120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "with torch.no_grad():\n",
    "    images, targets = next(iter(train_loader))\n",
    "    loss_box_matcher = matcher.HungarianMatcher(device='cuda').to('cuda')\n",
    "    prediction = detr(torch.rand((8,3,800,800), device='cuda'))\n",
    "    prediction['pred_boxes'] = prediction['pred_boxes'].permute(1, 0, 2)\n",
    "    prediction['pred_logits'] = prediction['pred_logits'].permute(1, 0, 2)\n",
    "    recon = loss_box_matcher.forward(prediction, targets)\n",
    "    pred_boxes = [prediction['pred_boxes'][k, recon[k][0], :] for k in range(prediction['pred_boxes'].shape[0])]\n",
    "    tg_boxes = [targets[k]['boxes'] for k in range(len(targets))]\n",
    "    pred_clasf = [prediction['pred_logits'][k, recon[k][1], :] for k in range(prediction['pred_logits'].shape[0])]\n",
    "    tg_clasf = [targets[k]['labels'] for k in range(len(targets))]\n",
    "    print(f\"{pred_boxes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6599fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(prediction['pred_logits'].shape[0]):\n",
    "#     print(f\"i: {i}, {recon[i][1]}\")\n",
    "#     print(prediction['pred_logits'][i, recon[i][1], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe4287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = torch.randn((8,32,91))\n",
    "# for i in range(prediction['pred_logits'].shape[0]):\n",
    "#     print(f\"recon: {recon[i][1]}, {prediction['pred_logits'][i, recon[i][1], :]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     pred = detr(torch.rand((32,3,800,800), device='cuda'))\n",
    "#     x = pred[\"pred_logits\"]\n",
    "#     classes = torch.argmax(pred[\"pred_logits\"], dim=-1)\n",
    "#     unique_classes = torch.unique(classes)\n",
    "#     print(unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_params = count_parameters(detr)\n",
    "space_mb = num_params * 4 / (1024 ** 2)\n",
    "print(f\"Parameters: {num_params}\")\n",
    "print(f\"Estimated space: {space_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = torch.tensor([0.05, 0.01])\n",
    "l2 = torch.tensor([0.3, 0.4])\n",
    "H = 4\n",
    "W = 2\n",
    "l1.repeat(H, 1, 1)[:H]\n",
    "# pos = torch.cat([l1.repeat(H, 1, 1)[:H].unsqueeze(0), l2.repeat(1, W, 1).unsqueeze(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd89e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = torch.rand((2, 2, 2)) # B C P -> P B C, B P C\n",
    "# k, k.flatten(2).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646881d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Playground for row and col embed in detr\n",
    "# H = 1\n",
    "# W = 2\n",
    "# hidden_dim = 8\n",
    "# row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "# col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "\n",
    "# pos = torch.cat([\n",
    "#  col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "#  row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "# ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "# row_embed[0], row_embed[:H].unsqueeze(1).repeat(1, W, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78aa99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227569c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = torch.tensor([[1,2,3,4], [4,5,6,7]])\n",
    "\n",
    "# H = 2\n",
    "# W = 2\n",
    "# # l1 = l.unsqueeze(0).repeat(H, 1, 1)\n",
    "# # l2 = l.unsqueeze(1).repeat(1, W, 1)\n",
    "\n",
    "# torch.cat([\n",
    "#     l.unsqueeze(0).repeat(H, 1, 1),\n",
    "#     l.unsqueeze(1).repeat(1, W, 1),\n",
    "# ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "# # l1.shape, l2.shape#, torch.cat([l1,l2], dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
